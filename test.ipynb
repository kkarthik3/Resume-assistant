{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoClient(host=['cluster0-shard-00-02.vgbtm.mongodb.net:27017', 'cluster0-shard-00-01.vgbtm.mongodb.net:27017', 'cluster0-shard-00-00.vgbtm.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin', replicaset='atlas-rb1oqm-shard-0', tls=True)\n",
      "Collection(Database(MongoClient(host=['cluster0-shard-00-02.vgbtm.mongodb.net:27017', 'cluster0-shard-00-01.vgbtm.mongodb.net:27017', 'cluster0-shard-00-00.vgbtm.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin', replicaset='atlas-rb1oqm-shard-0', tls=True), 'portfolio'), 'test')\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import os\n",
    "def connect_to_mongo():\n",
    "    client = MongoClient(os.getenv(\"MONGO_URI\"))\n",
    "    collection = client[\"portfolio\"][\"test\"]\n",
    "    return client, collection\n",
    "client,collection = connect_to_mongo()\n",
    "print(client)\n",
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.drop_database(\"portfolio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oorjit', 'pedalmobility', 'portfolio', 'admin', 'local']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 240, 'electionId': ObjectId('7fffffff0000000000000312'), 'opTime': {'ts': Timestamp(1731167278, 233), 't': 786}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1731167278, 242), 'signature': {'hash': b'\\x02\\x02\\xf8\\x946\\xb2z`\\xb3m\\x11X\\xd7\\xfe\\xee$\\xbb\\x95\\x8f\\x94', 'keyId': 7398866171417591816}}, 'operationTime': Timestamp(1731167278, 233)}, acknowledged=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\langraph_Tools\\langgraph\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'FSRCNN model increased image clarity by 35% enabling extraction of crucial details for accurate diagnosis and \\ntreatment planning. \\n• \\nDeveloped a Streamlit environment for interactive deployment, facilitating easy input of images and immediate \\nvisualization of enhanced outputs. This streamlined workflow led to a 14% improvement in real-time \\npredictions. \\n• \\nThe FSRCNN model with PReLU activation performs better compared to bicubic and ReLU by 20% and 2%, \\nrespectively.'},\n",
       " {'text': 'Langgraph tools and agents, enabling efficient, task-specific interactions by automating responses and decision-\\nmaking processes to enhance user experience and accuracy over 70% instead of manual flow approach. \\nPROJECTS'},\n",
       " {'text': '(NLP) framework, to analyze the sentiment of the scraped reviews. By utilizing a pre-trained model, the need \\nfor manual training of a sentiment analysis model from scratch was eliminated By 100% \\n• \\nUtilized ROBERTA model to analyze scraped reviews and classify sentiments (positive, negative, or neutral), \\nextracting valuable insights on customer opinions and preferences toward the products. \\nOTHER ACTIVITIES'},\n",
       " {'text': 'processing tasks, capable of handling more than 100,000 instances. \\n• \\nConducted fine-tuning of the model for Adaptation to Domain and Data using a custom classification dataset, \\nemploying techniques such as stopword removal and word embeddings for preprocessing.'},\n",
       " {'text': 'Data Wrangling    Data Preparation, Data Extraction, Data Cleaning, Exploratory Data Analysis(EDA), Data            \\nVisualization, Feature Engineering,Feature selection, Model Building, IoT . \\nMachine Learning        Data Modeling, Clustering & Classification, Quantitative Analysis, Regression, Transformers, \\n                                         Predictive Modeling, Model Validation, Model deployment, CNN & RNN, LSTM, BERT.'},\n",
       " {'text': 'SKILLS \\nLibraries                         Python (Numpy, Pandas, Matplotlib, Seaborn, Scikit-Learn, Tensorflow, Keras, NLTK, Streamlit),  \\n                                         R (Dplyr, ggplot), Fast-API, Flask, Langchain, Langgraph \\nLanguages                     Python (Intermediate), R (Fundamentals), SQL , Cypher (Neo4J)'},\n",
       " {'text': 'EXPERIENCE \\nMachine Learning Engineer Intern [ISPG Technologies Pvt.Ltd, Kochin, Kerala] [Link: https://www.ispg.co/]                                     May 2024 – Present \\n• \\nRAG Technology Integration: Integrated Retrieval-Augmented Generation (RAG) into the CRM database, \\nenabling the seamless translation of user NLP queries into Cypher queries and converting Cypher responses \\ninto human-readable formats. This integration enhanced user interaction and operational efficiency by 40%. \\n•'},\n",
       " {'text': 'Tools                               Jupyter Notebook, Arduino IDE, R Studio, MS Excel, Tableau, VS code, Power BI, Postman. \\nTechnologies                 Fundamentals (PySpark, Hive) , Langchain, LLM, Large Language Models, IoT. \\nCloud           AWS EC2, AWS IAM, AWS Beanstalk, AWS Bedrock, AWS RDS,  AWS DocumentDB,               \\nAWS Sagemaker, AWS APIGateway, AWS Lambda, AWS S3 \\n \\nEXPERIENCE'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.mongorag.query import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = get_query_results(\"what is your name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FSRCNN model increased image clarity by 35% enabling extraction of crucial details for accurate diagnosis and \\ntreatment planning. \\n• \\nDeveloped a Streamlit environment for interactive deployment, facilitating easy input of images and immediate \\nvisualization of enhanced outputs. This streamlined workflow led to a 14% improvement in real-time \\npredictions. \\n• \\nThe FSRCNN model with PReLU activation performs better compared to bicubic and ReLU by 20% and 2%, \\nrespectively.Langgraph tools and agents, enabling efficient, task-specific interactions by automating responses and decision-\\nmaking processes to enhance user experience and accuracy over 70% instead of manual flow approach. \\nPROJECTS(NLP) framework, to analyze the sentiment of the scraped reviews. By utilizing a pre-trained model, the need \\nfor manual training of a sentiment analysis model from scratch was eliminated By 100% \\n• \\nUtilized ROBERTA model to analyze scraped reviews and classify sentiments (positive, negative, or neutral), \\nextracting valuable insights on customer opinions and preferences toward the products. \\nOTHER ACTIVITIESprocessing tasks, capable of handling more than 100,000 instances. \\n• \\nConducted fine-tuning of the model for Adaptation to Domain and Data using a custom classification dataset, \\nemploying techniques such as stopword removal and word embeddings for preprocessing.Data Wrangling    Data Preparation, Data Extraction, Data Cleaning, Exploratory Data Analysis(EDA), Data            \\nVisualization, Feature Engineering,Feature selection, Model Building, IoT . \\nMachine Learning        Data Modeling, Clustering & Classification, Quantitative Analysis, Regression, Transformers, \\n                                         Predictive Modeling, Model Validation, Model deployment, CNN & RNN, LSTM, BERT.SKILLS \\nLibraries                         Python (Numpy, Pandas, Matplotlib, Seaborn, Scikit-Learn, Tensorflow, Keras, NLTK, Streamlit),  \\n                                         R (Dplyr, ggplot), Fast-API, Flask, Langchain, Langgraph \\nLanguages                     Python (Intermediate), R (Fundamentals), SQL , Cypher (Neo4J)EXPERIENCE \\nMachine Learning Engineer Intern [ISPG Technologies Pvt.Ltd, Kochin, Kerala] [Link: https://www.ispg.co/]                                     May 2024 – Present \\n• \\nRAG Technology Integration: Integrated Retrieval-Augmented Generation (RAG) into the CRM database, \\nenabling the seamless translation of user NLP queries into Cypher queries and converting Cypher responses \\ninto human-readable formats. This integration enhanced user interaction and operational efficiency by 40%. \\n•Tools                               Jupyter Notebook, Arduino IDE, R Studio, MS Excel, Tableau, VS code, Power BI, Postman. \\nTechnologies                 Fundamentals (PySpark, Hive) , Langchain, LLM, Large Language Models, IoT. \\nCloud           AWS EC2, AWS IAM, AWS Beanstalk, AWS Bedrock, AWS RDS,  AWS DocumentDB,               \\nAWS Sagemaker, AWS APIGateway, AWS Lambda, AWS S3 \\n \\nEXPERIENCE'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(i[\"text\"] for i in hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
